{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12269782,"sourceType":"datasetVersion","datasetId":7731899},{"sourceId":12272719,"sourceType":"datasetVersion","datasetId":7734022}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport nibabel as nib\nimport os\nimport re\nimport xml.etree.ElementTree as ET\nfrom tqdm import tqdm\nfrom scipy.stats import entropy\nfrom skimage.exposure import histogram\nimport cv2","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:10:15.644333Z","iopub.execute_input":"2025-06-26T17:10:15.644647Z","iopub.status.idle":"2025-06-26T17:10:18.526956Z","shell.execute_reply.started":"2025-06-26T17:10:15.644617Z","shell.execute_reply":"2025-06-26T17:10:18.525998Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/adni-processed/ADNI1_Complete_1Yr_1.5T_6_20_2025.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:10:18.528456Z","iopub.execute_input":"2025-06-26T17:10:18.529002Z","iopub.status.idle":"2025-06-26T17:10:18.570981Z","shell.execute_reply.started":"2025-06-26T17:10:18.528971Z","shell.execute_reply":"2025-06-26T17:10:18.569994Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import os\n\ndef get_all_i_folders_kaggle(root_path):\n    \"\"\"\n    Recursively find all folders that start with 'I' and return their names.\n    \"\"\"\n    i_folders = []\n    for dirpath, dirnames, filenames in os.walk(root_path):\n        for dirname in dirnames:\n            if dirname.startswith('I') and dirname[1:].isdigit():\n                i_folders.append(dirname)\n    return i_folders\n\n# Usage\nkaggle_root = \"/kaggle/input/adni-processed/ADNI1_Processed/ADNI1_Processed\"\ni_folders = get_all_i_folders_kaggle(kaggle_root)\n\nprint(f\"Found {len(i_folders)} I folders:\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:10:18.571946Z","iopub.execute_input":"2025-06-26T17:10:18.572256Z","iopub.status.idle":"2025-06-26T17:10:24.629686Z","shell.execute_reply.started":"2025-06-26T17:10:18.572231Z","shell.execute_reply":"2025-06-26T17:10:24.628701Z"}},"outputs":[{"name":"stdout","text":"Found 459 I folders:\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/adni-processed/ADNI1_Complete_1Yr_1.5T_6_20_2025.csv\")\nfolder_path = \"/kaggle/input/adni-processed/ADNI1_Processed/ADNI1_Processed\"\npaths = []\nfolder_2 = \"/kaggle/input/metadata\"\n\nfor root_dir, dirs, files in tqdm(os.walk(folder_path), desc=\"Scanning files\"):\n    for file in files:\n        if file.endswith(\".nii\") or file.endswith(\".nii.gz\"):\n            final_path = os.path.join(root_dir, file)\n            rel_path = os.path.relpath(final_path, folder_path)\n            \n            # Extract subject and image ID using regex\n            match = re.search(r'_S(\\d+)_I(\\d+)', file)\n            if match:\n                s_num = match.group(1)\n                i_num = match.group(2)\n                new_filename = f\"S{s_num}I{i_num}.xml\"\n\n                nii_dir = os.path.dirname(rel_path)\n                xml_path = os.path.join(folder_2, new_filename)\n\n                if os.path.exists(xml_path):\n                    try:\n                        tree = ET.parse(xml_path)\n                        xml_root = tree.getroot()\n                        id = xml_root[3].attrib.get('uid', None)\n\n                        if id:\n                            row = df[df['Image Data ID'].astype(str).str.strip() == str(id).strip()]\n                            if not row.empty:\n                                label = row.iloc[0, 2]\n                                paths.append((label, final_path))\n                            else:\n                                print(f\"[!] ID {id} not found in DataFrame\")\n                        else:\n                            print(f\"[!] UID not found in XML: {xml_path}\")\n                    except Exception as e:\n                        print(f\"[!] Failed to parse XML: {xml_path} — {e}\")\n                else:\n                    print(f\"[!] XML file missing: {xml_path}\")\n            else:\n                print(f\"[!] Failed to extract subject/image ID from: {file}\")\n\nfiltered_paths = []\nfor path in paths:\n    if path[0] in ('AD', 'CN'):\n        filtered_paths.append(path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:10:24.631848Z","iopub.execute_input":"2025-06-26T17:10:24.632178Z","iopub.status.idle":"2025-06-26T17:10:28.927836Z","shell.execute_reply.started":"2025-06-26T17:10:24.632153Z","shell.execute_reply":"2025-06-26T17:10:28.926694Z"}},"outputs":[{"name":"stderr","text":"Scanning files: 815it [00:04, 190.99it/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"X = []\ny = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:10:28.928837Z","iopub.execute_input":"2025-06-26T17:10:28.929164Z","iopub.status.idle":"2025-06-26T17:10:28.934003Z","shell.execute_reply.started":"2025-06-26T17:10:28.929142Z","shell.execute_reply":"2025-06-26T17:10:28.933037Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def center_crop(image, crop_size=128):\n    h, w = image.shape\n    if h < crop_size or w < crop_size:\n        return None\n    top = (h - crop_size) // 2\n    left = (w - crop_size) // 2\n    return image[top:top+crop_size, left:left+crop_size]\n\ndef image_entropy(img):\n    hist, _ = histogram(img)\n    hist = hist / np.sum(hist)\n    return entropy(hist, base=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:10:28.934725Z","iopub.execute_input":"2025-06-26T17:10:28.935011Z","iopub.status.idle":"2025-06-26T17:10:28.959850Z","shell.execute_reply.started":"2025-06-26T17:10:28.934991Z","shell.execute_reply":"2025-06-26T17:10:28.958437Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Taking only axial slices","metadata":{}},{"cell_type":"code","source":"N = 25  # number of entropy-based slices\ncrop_size = 256\n\nfor label, path in tqdm(filtered_paths):\n    scan = nib.load(path)\n    data = scan.get_fdata()\n    label = 0 if label == 'AD' else 1\n\n    slice_info = []\n\n\n    for i in range(data.shape[2]):  # only axial\n        slice_ = data[:, :, i]\n        # Crop and skip empty ones\n        cropped = center_crop(slice_, crop_size=crop_size)\n        if cropped is None:\n            continue\n\n        # Compute entropy\n        ent = image_entropy(cropped)\n        slice_info.append((ent, cropped))\n\n    # for axis in [0, 1, 2]:  # optionally restrict to one axis\n    #     for i in range(data.shape[axis]):\n    #         # Extract 2D slice along the given axis\n    #         if axis == 0:\n    #             slice_ = data[i, :, :]\n    #         elif axis == 1:\n    #             slice_ = data[:, i, :]\n    #         else:\n    #             slice_ = data[:, :, i]\n\n    #         # Crop and skip empty ones\n    #         cropped = center_crop(slice_, crop_size=crop_size)\n    #         if cropped is None:\n    #             continue\n\n    #         # Compute entropy\n    #         ent = image_entropy(cropped)\n    #         slice_info.append((ent, cropped))\n\n    # Sort slices by entropy\n    slice_info.sort(reverse=True, key=lambda x: x[0])\n    top_slices = slice_info[:N]\n\n    # If not enough valid slices, skip this subject\n    if len(top_slices) < N:\n        print(f\"[!] Skipped subject: only {len(top_slices)} slices\")\n        continue\n\n    # Build per-subject volume\n    subject_volume = [s[1][..., np.newaxis] for s in top_slices]  \n    subject_volume = np.stack(subject_volume, axis=0)  \n\n    X.append(subject_volume)\n    y.append(label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:10:28.961139Z","iopub.execute_input":"2025-06-26T17:10:28.961527Z","iopub.status.idle":"2025-06-26T17:14:16.805820Z","shell.execute_reply.started":"2025-06-26T17:10:28.961495Z","shell.execute_reply":"2025-06-26T17:14:16.804693Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 234/234 [03:47<00:00,  1.03it/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"len(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:14:16.807446Z","iopub.execute_input":"2025-06-26T17:14:16.807786Z","iopub.status.idle":"2025-06-26T17:14:16.817129Z","shell.execute_reply.started":"2025-06-26T17:14:16.807757Z","shell.execute_reply":"2025-06-26T17:14:16.815694Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"234"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"X[0].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:14:16.818403Z","iopub.execute_input":"2025-06-26T17:14:16.818775Z","iopub.status.idle":"2025-06-26T17:14:16.842827Z","shell.execute_reply.started":"2025-06-26T17:14:16.818749Z","shell.execute_reply":"2025-06-26T17:14:16.842028Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(25, 256, 256, 1)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\n\nX_flat = []\ny_flat = []\n\n\nfor i in range(len(X)):\n    volume = X[i]  \n    label = y[i]\n    for j in range(25):\n        slice_2d = volume[j, :, :, 0]     \n        X_flat.append(slice_2d)\n        y_flat.append(label)\n\n# Convert to NumPy arrays\nX_flat = np.array(X_flat) \ny_flat = np.array(y_flat)  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:14:16.845829Z","iopub.execute_input":"2025-06-26T17:14:16.846168Z","iopub.status.idle":"2025-06-26T17:14:18.844928Z","shell.execute_reply.started":"2025-06-26T17:14:16.846144Z","shell.execute_reply":"2025-06-26T17:14:18.843800Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"X_flat.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:14:18.845708Z","iopub.execute_input":"2025-06-26T17:14:18.846087Z","iopub.status.idle":"2025-06-26T17:14:18.852944Z","shell.execute_reply.started":"2025-06-26T17:14:18.846055Z","shell.execute_reply":"2025-06-26T17:14:18.851987Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(5850, 256, 256)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"To produce graphs for SGCNN","metadata":{}},{"cell_type":"code","source":"# --- CONFIG ---\nGRAPH_DIR = \"brain_graphs\"\n\n# --- 1. GRAPH GENERATION FROM FLAT SLICE LIST ---\nimport os\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom skimage.measure import regionprops\nfrom skimage.exposure import histogram\nfrom scipy.stats import entropy\nfrom torch_geometric.data import Data\nfrom torch_geometric.utils import to_dense_adj\nfrom sklearn.cluster import KMeans\n\n\nCROP_SIZE = 128\nos.makedirs(GRAPH_DIR, exist_ok=True)\n\ndef segment_slice_kmeans(slice_img, n_regions=5):\n    flat = slice_img.reshape(-1, 1)\n    km = KMeans(n_clusters=n_regions, n_init='auto').fit(flat)\n    return km.labels_.reshape(slice_img.shape)\n\ndef slice_to_graph(slice_img, mask, label):\n    num_nodes = int(mask.max()) + 1\n    x = []\n    props = regionprops(mask, intensity_image=slice_img)\n    for region in props:\n        x.append([\n            region.mean_intensity,\n            region.area,\n            region.eccentricity,\n            region.solidity,\n            region.perimeter,\n            region.extent\n        ])\n    x = torch.tensor(x, dtype=torch.float)\n    adj = np.zeros((num_nodes, num_nodes))\n    for i in range(num_nodes):\n        for j in range(i + 1, num_nodes):\n            touching = (\n                np.any((mask == i) & np.roll(mask == j, 1, axis=0)) or\n                np.any((mask == i) & np.roll(mask == j, 1, axis=1))\n            )\n            if touching:\n                adj[i, j] = adj[j, i] = 1\n    edge_index = torch.tensor(np.array(np.nonzero(adj)), dtype=torch.long)\n    return Data(x=x, edge_index=edge_index, y=torch.tensor([label], dtype=torch.long))\n\nfor idx, (slice_img, label) in enumerate(tqdm(zip(X_flat, y_flat), total=len(X_flat))):\n    img = slice_img.squeeze()  # remove channel if present\n    mask = segment_slice_kmeans(img)\n    g = slice_to_graph(img, mask, label)\n    torch.save(g, os.path.join(GRAPH_DIR, f\"slice_{idx}.pt\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T17:16:30.675119Z","iopub.execute_input":"2025-06-26T17:16:30.675468Z","iopub.status.idle":"2025-06-26T17:22:18.214480Z","shell.execute_reply.started":"2025-06-26T17:16:30.675438Z","shell.execute_reply":"2025-06-26T17:22:18.213344Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 5850/5850 [05:40<00:00, 17.19it/s]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"\nGRAPH_DIR = \"brain_graphs\"\n\n\nimport os\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\nfrom skimage.measure import regionprops\nfrom skimage.exposure import histogram\nfrom scipy.stats import entropy\nfrom torch_geometric.data import Data, Dataset\nfrom torch_geometric.utils import to_dense_adj\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv, global_mean_pool\nfrom torch_geometric.loader import DataLoader as GraphDataLoader\nfrom torch.utils.data import DataLoader as ImageDataLoader, Dataset as TorchDataset\n\n# --- Hybrid Model Definition ---\nclass CNNBranch(nn.Module):\n    def __init__(self):\n        super(CNNBranch, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n        self.pool = nn.MaxPool2d(2)\n        self.adapt_pool = nn.AdaptiveAvgPool2d((16, 16))\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(64 * 16 * 16, 128)\n\n    def forward(self, x):\n        x = F.relu(self.pool(self.conv1(x)))\n        x = F.relu(self.pool(self.conv2(x)))\n        x = F.relu(self.pool(self.conv3(x)))\n        x = self.adapt_pool(x)\n        x = self.flatten(x)\n        return self.fc(x)\n\nclass SGCNNBranch(nn.Module):\n    def __init__(self, in_features=6):\n        super(SGCNNBranch, self).__init__()\n        self.conv1 = GCNConv(in_features, 32)\n        self.conv2 = GCNConv(32, 64)\n        self.pool = global_mean_pool\n        self.fc = nn.Linear(64, 128)\n\n    def forward(self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        x = F.relu(self.conv1(x, edge_index))\n        x = F.relu(self.conv2(x, edge_index))\n        x = self.pool(x, batch)\n        return self.fc(x)\n\nclass HybridClassifier(nn.Module):\n    def __init__(self):\n        super(HybridClassifier, self).__init__()\n        self.cnn_branch = CNNBranch()\n        self.sgcnn_branch = SGCNNBranch()\n        self.classifier = nn.Linear(256, 2)\n\n    def forward(self, image, graph):\n        cnn_feat = self.cnn_branch(image)\n        sgcnn_feat = self.sgcnn_branch(graph)\n        combined = torch.cat([cnn_feat, sgcnn_feat], dim=1)\n        return self.classifier(combined)\n\n#different datasets classes for sgcnn and cnn model\nclass GraphOnlyDataset(Dataset):\n    def __init__(self, graph_dir, indices):\n        super().__init__()\n        self.graph_paths = sorted([os.path.join(graph_dir, f) for f in os.listdir(graph_dir) if f.endswith(\".pt\")])\n        self.graph_paths = [self.graph_paths[i] for i in indices]\n\n    def len(self):\n        return len(self.graph_paths)\n\n    def get(self, idx):\n        return torch.load(self.graph_paths[idx], weights_only=False)\n\nclass ImageOnlyDataset(TorchDataset):\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        return self.images[idx], self.labels[idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}